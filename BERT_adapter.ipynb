{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw5_problem5_adapter_fusion.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HywR0S12rPDX"},"source":["## Problem 5"]},{"cell_type":"code","metadata":{"id":"KC9WZDt3xkPM"},"source":["## For this question, fill in the TODO's "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmqCEJrUbi0k"},"source":["!pip install git+https://github.com/adapter-hub/adapter-transformers.git\n","!git clone https://github.com/huggingface/transformers\n","!python transformers/utils/download_glue_data.py --tasks RTE\n","\n","import dataclasses\n","import logging\n","import os\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Dict, Optional\n","\n","import numpy as np\n","\n","import torch\n","from transformers import AutoTokenizer, EvalPrediction, GlueDataset, GlueDataTrainingArguments, AutoModelWithHeads, AdapterType, AutoConfig, AutoModelForSequenceClassification\n","from transformers import GlueDataTrainingArguments as DataTrainingArguments\n","from transformers import (\n","    Trainer,\n","    TrainingArguments,\n","    glue_compute_metrics,\n","    glue_tasks_num_labels,\n","    set_seed,\n",")\n","\n","model_name = # TODO \n","\n","\n","# Refer to the notebook for training an adapter to write these. Set the number of epochs to 3, and learning rate to 5e-5. Rest of the hyperparameters can stay the same. \n","\n","data_args = # TODO \n","\n","training_args = # TODO\n",")\n","\n","# TODO: Change this seed when re-running your code to report the mean and std dev\n","set_seed(100)\n","num_labels = glue_tasks_num_labels[data_args.task_name]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JRwqK_-TbPU","outputId":"64ea92df-d32d-4188-bf9c-839bb03de1fc","colab":{"base_uri":"https://localhost:8080/"}},"source":["config = AutoConfig.from_pretrained(\n","        model_name,\n","        num_labels=num_labels,\n","        finetuning_task=data_args.task_name,\n","        cache_dir=\".\",\n","    )\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_name,\n","    cache_dir=\".\",\n",")\n","\n","model = AutoModelWithHeads.from_pretrained(model_name, config=config)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"QxY1ayPk340s"},"source":["Now we have everything set up to load our AdapterFusion setup. \n","\n","First, you need to go to adapterhub.ml and explore the different adapters available. Choose any three adapters and load the three adapters. As we don't need their prediction heads, we pass with_head=False to the loading method. Next, we add a new fusion layer that combines all the adapters we've just loaded. Finally, we add a new classification head for our target task on top."]},{"cell_type":"code","metadata":{"id":"rxE-ebUo37jm"},"source":["# First, load the pre-trained adapters we want to fuse from Hub\n","from transformers.adapter_config import PfeifferConfig\n","\n","model.load_adapter(\"nli/rte@ukp\", \"text_task\", config=PfeifferConfig(), load_as='rte', with_head=False)\n","# TODO: load some more adapters. Choose the ones that you think will help RTE task after reading about the different tasks available and how big the datasets are.\n","\n","\n","# Add a fusion layer and tell the model to train fusion (freezes the rest of the weights) (here can either add the actual atsk adapter or not)\n","model.add_fusion([\n","        \"rte\",\n","        # TODO: Add your other task names here for the adapters you chose\n","    ])\n","\n","# Add a classification head for our target task\n","# TODO: Check the earlier notebook from Problem 5 to see how to add a classification head for your task."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1G4aQBqn3pkl"},"source":["The last preparation step is to define and activate our adapter setup. Similar to train_adapter(), train_fusion() does two things: It freezes all weights of the model (including adapters!) except for the fusion layer and classification head. It also activates the given adapter setup to be used in very forward pass.\n","\n","The syntax for the adapter setup (which is also applied to other methods such as set_active_adapters()) works as follows:\n","\n","a single string is interpreted as a single adapter, \n","a list of strings is interpreted as a stack of adapters,\n","a nested list of strings is interpreted as a fusion of adapters. Here want to do Fusion so we use a nested list as follows. \n"]},{"cell_type":"code","metadata":{"id":"aLgNXXBi3sE7"},"source":["adapter_setup = [\n","                 [\n","        \"rte\",\n","        # TODO: Add your other adapter names here. \n","    ]\n","]\n","model.train_fusion(adapter_setup)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FS-UUxG3u2zN","outputId":"3099d2b8-beeb-4457-b446-e9821f615be4","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Check out your training args\n","print(training_args)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TrainingArguments(output_dir='./models/rte', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov13_22-37-17_8bd483eefc80', logging_first_step=False, logging_steps=50, save_steps=1000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"l128JFN3Vr9n"},"source":["train_dataset = GlueDataset(data_args, tokenizer=tokenizer)\n","eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\")\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = np.argmax(p.predictions, axis=1)\n","    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)\n","\n","trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics,\n","        do_save_full_model=False,\n","        do_save_adapter_fusion=True,\n","    )\n","\n","trainer.train()\n","trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaO2ET8Eu-fE"},"source":[""],"execution_count":null,"outputs":[]}]}